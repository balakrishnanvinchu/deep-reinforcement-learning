{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balakrishnanvinchu/deep-reinforcement-learning/blob/main/DQN_and_DDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `---------------Mandatory Information to fill------------`"
      ],
      "metadata": {
        "id": "j7jO_ata_tGB"
      },
      "id": "j7jO_ata_tGB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Group ID:\n",
        "### Group Members Name with Student ID:\n",
        "1. Student 1\n",
        "2. Student 2\n",
        "3. Student 3\n",
        "4. Student 4"
      ],
      "metadata": {
        "id": "ilJIt-lgIp04"
      },
      "id": "ilJIt-lgIp04"
    },
    {
      "cell_type": "markdown",
      "source": [
        "`-------------------Write your remarks (if any) that you want should get consider at the time of evaluation---------------`"
      ],
      "metadata": {
        "id": "YXHhoNgkAhUg"
      },
      "id": "YXHhoNgkAhUg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remarks: ##Add here"
      ],
      "metadata": {
        "id": "-5tK16CbA5X_"
      },
      "id": "-5tK16CbA5X_"
    },
    {
      "cell_type": "markdown",
      "id": "cc769279",
      "metadata": {
        "id": "cc769279"
      },
      "source": [
        "## Autonomous Drone Battery Management for Urban Surveillance using DQN and DDQN - 7 Marks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9229188",
      "metadata": {
        "id": "f9229188"
      },
      "source": [
        "### Import Statements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "893bf5df",
      "metadata": {
        "id": "893bf5df"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment Parameters\n",
        "GRID_SIZE = (10, 10)\n",
        "BATTERY_CAPACITY = 100.0\n",
        "INITIAL_BATTERY = 100.0\n",
        "\n",
        "# Action Costs\n",
        "BASE_MOVE_COST = 0.5\n",
        "BASE_HOVER_COST = 0.2\n",
        "RECHARGE_RATE = 5.0\n",
        "\n",
        "# Reward Structure\n",
        "BATTERY_CRASH_PENALTY = -100.0\n",
        "TIME_PENALTY = -0.1\n",
        "RECHARGE_BONUS = 1.0\n",
        "\n",
        "# POI Parameters\n",
        "POI_SPAWN_CHANCE = 0.05\n",
        "MAX_ACTIVE_POIS = 3\n",
        "POI_LIFESPAN_RANGE = (10, 30)\n",
        "POI_VALUE_RANGE = (10, 50)\n",
        "\n",
        "# Atmospheric Disturbance Parameters\n",
        "DISTURBANCE_CHANGE_PROB = 0.1\n",
        "DISTURBANCE_MAGNITUDE_CHANGE = 0.05\n",
        "DISTURBANCE_FACTOR = 0.5\n",
        "\n",
        "# Agent Configuration\n",
        "STATE_SIZE = 7\n",
        "ACTION_SIZE = 6\n",
        "\n",
        "# Learning Parameters\n",
        "LEARNING_RATE = 0.001\n",
        "DISCOUNT_FACTOR = 0.95\n",
        "REPLAY_BUFFER_SIZE = 5000\n",
        "MIN_REPLAY_SIZE = 1000\n",
        "BATCH_SIZE = 34\n",
        "\n",
        "# Exploration Strategy\n",
        "EXPLORATION_MAX = 1.0\n",
        "EXPLORATION_MIN = 0.01\n",
        "EXPLORATION_DECAY = 0.995\n",
        "TARGET_UPDATE_FREQUENCY = 10\n",
        "\n",
        "# Training Setup\n",
        "EPISODES = 200\n",
        "MAX_TIMESTEPS_PER_EPISODE = 100"
      ],
      "metadata": {
        "id": "pA7nkPC3yXLy"
      },
      "id": "pA7nkPC3yXLy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc3d02d5",
      "metadata": {
        "id": "dc3d02d5"
      },
      "outputs": [],
      "source": [
        "# --- 2. Replay Buffer Class ---\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size):\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "    def add(self, experience):\n",
        "        \"\"\"Add new experience to buffer\"\"\"\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Randomly sample a batch of experiences\"\"\"\n",
        "        return random.sample(self.buffer, min(len(self.buffer), batch_size))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84904415",
      "metadata": {
        "id": "84904415"
      },
      "source": [
        "### --- 3. Custom Environment: DroneSurveillanceEnv --- - 2 Marks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DroneSurveillanceEnv:\n",
        "    def __init__(self, grid_size, battery_capacity, initial_battery):\n",
        "        \"\"\"Initialize the drone surveillance environment\"\"\"\n",
        "        self.grid_size = grid_size\n",
        "        self.battery_capacity = battery_capacity\n",
        "        self.initial_battery = initial_battery\n",
        "\n",
        "        # Drone State\n",
        "        self.drone_pos = [0, 0]  # Start at origin\n",
        "        self.battery_level = initial_battery\n",
        "\n",
        "        # Environmental Dynamics\n",
        "        self.disturbance = 0.0  # Initial atmospheric disturbance level (0-1)\n",
        "        self.pois = []  # Active POIs: [x, y, value, remaining_lifespan]\n",
        "        self.time_step = 0\n",
        "\n",
        "        # Charging Stations (fixed at grid corners)\n",
        "        self.charging_stations = [\n",
        "            [0, 0],\n",
        "            [0, grid_size[1]-1],\n",
        "            [grid_size[0]-1, 0],\n",
        "            [grid_size[0]-1, grid_size[1]-1]\n",
        "        ]\n",
        "\n",
        "        # Action mapping\n",
        "        self.action_map = {\n",
        "            0: [0, 1],   # North\n",
        "            1: [0, -1],  # South\n",
        "            2: [1, 0],   # East\n",
        "            3: [-1, 0],  # West\n",
        "            4: [0, 0],   # Hover\n",
        "            5: [0, 0]    # Recharge\n",
        "        }\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset environment to initial state\"\"\"\n",
        "        self.drone_pos = [0, 0]\n",
        "        self.battery_level = self.initial_battery\n",
        "        self.disturbance = 0.0\n",
        "        self.pois = []\n",
        "        self.time_step = 0\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        \"\"\"Convert environment state to observation vector for NN\"\"\"\n",
        "        # Find nearest POI\n",
        "        nearest_poi = None\n",
        "        min_dist = float('inf')\n",
        "        poi_value = 0.0\n",
        "        poi_lifespan = 0.0\n",
        "\n",
        "        for poi in self.pois:\n",
        "            dist = np.sqrt((poi[0]-self.drone_pos[0])**2 + (poi[1]-self.drone_pos[1])**2)\n",
        "            if dist < min_dist:\n",
        "                min_dist = dist\n",
        "                nearest_poi = poi\n",
        "                poi_value = poi[2]\n",
        "                poi_lifespan = poi[3]\n",
        "\n",
        "        # Normalize all values for NN input\n",
        "        max_distance = np.sqrt(self.grid_size[0]**2 + self.grid_size[1]**2)\n",
        "        obs = [\n",
        "            self.drone_pos[0] / (self.grid_size[0]-1),  # Normalized x position\n",
        "            self.drone_pos[1] / (self.grid_size[1]-1),  # Normalized y position\n",
        "            self.battery_level / self.battery_capacity,  # Battery percentage\n",
        "            self.disturbance,  # Atmospheric disturbance (0-1)\n",
        "            min_dist / max_distance if nearest_poi else 1.0,  # Normalized distance\n",
        "            poi_value / POI_VALUE_RANGE[1] if nearest_poi else 0.0,  # Normalized value\n",
        "            poi_lifespan / POI_LIFESPAN_RANGE[1] if nearest_poi else 0.0  # Normalized lifespan\n",
        "        ]\n",
        "        return np.array(obs)\n",
        "\n",
        "    def _spawn_poi(self):\n",
        "        \"\"\"Randomly spawn new POIs according to spawn chance\"\"\"\n",
        "        if len(self.pois) >= MAX_ACTIVE_POIS:\n",
        "            return\n",
        "\n",
        "        if random.random() < POI_SPAWN_CHANCE:\n",
        "            # Generate random position not occupied by drone, charging station, or existing POI\n",
        "            while True:\n",
        "                x = random.randint(0, self.grid_size[0]-1)\n",
        "                y = random.randint(0, self.grid_size[1]-1)\n",
        "                pos = [x, y]\n",
        "\n",
        "                # Check if position is available\n",
        "                occupied = False\n",
        "                if pos == self.drone_pos:\n",
        "                    occupied = True\n",
        "                for station in self.charging_stations:\n",
        "                    if pos == station:\n",
        "                        occupied = True\n",
        "                for poi in self.pois:\n",
        "                    if pos == poi[:2]:\n",
        "                        occupied = True\n",
        "\n",
        "                if not occupied:\n",
        "                    value = random.uniform(POI_VALUE_RANGE[0], POI_VALUE_RANGE[1])\n",
        "                    lifespan = random.randint(POI_LIFESPAN_RANGE[0], POI_LIFESPAN_RANGE[1])\n",
        "                    self.pois.append([x, y, value, lifespan])\n",
        "                    break\n",
        "\n",
        "    def _update_pois(self, drone_at_poi_pos):\n",
        "        \"\"\"Update POI states and handle collection\"\"\"\n",
        "        reward = 0.0\n",
        "        new_pois = []\n",
        "\n",
        "        for poi in self.pois:\n",
        "            # Check if drone is at this POI's position\n",
        "            if drone_at_poi_pos and poi[0] == self.drone_pos[0] and poi[1] == self.drone_pos[1]:\n",
        "                reward += poi[2]  # Collect POI value\n",
        "            else:\n",
        "                # Decrement lifespan and keep if still active\n",
        "                poi[3] -= 1\n",
        "                if poi[3] > 0:\n",
        "                    new_pois.append(poi)\n",
        "\n",
        "        self.pois = new_pois\n",
        "        return reward\n",
        "\n",
        "    def _update_atmospheric_disturbance(self):\n",
        "        \"\"\"Randomly update atmospheric disturbance level\"\"\"\n",
        "        if random.random() < DISTURBANCE_CHANGE_PROB:\n",
        "            # Random walk disturbance\n",
        "            change = random.uniform(-DISTURBANCE_MAGNITUDE_CHANGE, DISTURBANCE_MAGNITUDE_CHANGE)\n",
        "            self.disturbance = np.clip(self.disturbance + change, 0.0, 1.0)\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Execute one time step in the environment\"\"\"\n",
        "        done = False\n",
        "        reward = TIME_PENALTY  # Default time penalty\n",
        "\n",
        "        # Update atmospheric disturbance\n",
        "        self._update_atmospheric_disturbance()\n",
        "\n",
        "        # Handle battery changes based on action\n",
        "        if action == 5:  # Recharge\n",
        "            # Check if at charging station\n",
        "            at_station = any(self.drone_pos == station for station in self.charging_stations)\n",
        "            if at_station:\n",
        "                self.battery_level = min(self.battery_capacity,\n",
        "                                       self.battery_level + RECHARGE_RATE)\n",
        "                reward += RECHARGE_BONUS\n",
        "        else:\n",
        "            # Calculate movement cost with disturbance factor\n",
        "            cost_multiplier = 1 + self.disturbance * DISTURBANCE_FACTOR\n",
        "            if action == 4:  # Hover\n",
        "                battery_cost = BASE_HOVER_COST * cost_multiplier\n",
        "            else:  # Movement\n",
        "                battery_cost = BASE_MOVE_COST * cost_multiplier\n",
        "\n",
        "            self.battery_level -= battery_cost\n",
        "\n",
        "            # Execute movement\n",
        "            if action in [0, 1, 2, 3]:  # Movement actions\n",
        "                move = self.action_map[action]\n",
        "                new_x = np.clip(self.drone_pos[0] + move[0], 0, self.grid_size[0]-1)\n",
        "                new_y = np.clip(self.drone_pos[1] + move[1], 0, self.grid_size[1]-1)\n",
        "                self.drone_pos = [new_x, new_y]\n",
        "\n",
        "        # Check for battery crash\n",
        "        if self.battery_level <= 0:\n",
        "            if not any(self.drone_pos == station for station in self.charging_stations):\n",
        "                reward += BATTERY_CRASH_PENALTY\n",
        "                done = True\n",
        "            self.battery_level = 0.0\n",
        "\n",
        "        # Update POIs and collect rewards\n",
        "        drone_at_poi = any(self.drone_pos == poi[:2] for poi in self.pois)\n",
        "        reward += self._update_pois(drone_at_poi)\n",
        "\n",
        "        # Spawn new POIs\n",
        "        self._spawn_poi()\n",
        "\n",
        "        # Increment time step\n",
        "        self.time_step += 1\n",
        "\n",
        "        return self._get_obs(), reward, done, {}\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Visualize the current environment state\"\"\"\n",
        "        grid = np.zeros(self.grid_size, dtype=str)\n",
        "        grid.fill('.')  # Empty cells\n",
        "\n",
        "        # Mark charging stations\n",
        "        for x, y in self.charging_stations:\n",
        "            grid[x, y] = 'C'\n",
        "\n",
        "        # Mark active POIs\n",
        "        for poi in self.pois:\n",
        "            x, y, val, life = poi\n",
        "            grid[x, y] = f'P{int(val)}'\n",
        "\n",
        "        # Mark drone position\n",
        "        x, y = self.drone_pos\n",
        "        grid[x, y] = 'D' if grid[x, y] == '.' else grid[x, y] + 'D'\n",
        "\n",
        "        # Print grid\n",
        "        print(f\"Time: {self.time_step} | Battery: {self.battery_level:.1f}% | Disturbance: {self.disturbance:.2f}\")\n",
        "        for row in grid.T:\n",
        "            print(' '.join(row))\n",
        "        print()"
      ],
      "metadata": {
        "id": "2Eyn249xxFVb"
      },
      "id": "2Eyn249xxFVb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, learning_rate, use_ddqn=False):\n",
        "        \"\"\"Initialize the DQN agent\"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.use_ddqn = use_ddqn\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Main Q-Network and Target Network\n",
        "        self.q_network = self._build_model()\n",
        "        self.target_network = self._build_model()\n",
        "        self.update_target_network()  # Initialize target network weights\n",
        "\n",
        "        # Experience replay buffer\n",
        "        self.memory = deque(maxlen=REPLAY_BUFFER_SIZE)\n",
        "\n",
        "        # Exploration parameters\n",
        "        self.epsilon = EXPLORATION_MAX\n",
        "        self.epsilon_min = EXPLORATION_MIN\n",
        "        self.epsilon_decay = EXPLORATION_DECAY\n",
        "\n",
        "        # Training parameters\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.discount_factor = DISCOUNT_FACTOR\n",
        "        self.target_update_frequency = TARGET_UPDATE_FREQUENCY\n",
        "        self.train_step_counter = 0\n",
        "\n",
        "    def _build_model(self):\n",
        "        \"\"\"Build neural network for Q-value approximation\"\"\"\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(64, input_dim=self.state_size, activation='relu'),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
        "        ])\n",
        "        model.compile(\n",
        "            loss='mse',\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"Update target network weights with Q-network weights\"\"\"\n",
        "        self.target_network.set_weights(self.q_network.get_weights())\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"Select action using epsilon-greedy policy\"\"\"\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)  # Random action\n",
        "\n",
        "        # Predict Q-values and select best action\n",
        "        state = np.reshape(state, [1, self.state_size])\n",
        "        q_values = self.q_network.predict(state, verbose=0)\n",
        "        return np.argmax(q_values[0])  # Best action\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Train the Q-network using experience replay\"\"\"\n",
        "        # Only train if we have enough experiences\n",
        "        if len(self.memory) < MIN_REPLAY_SIZE:\n",
        "            return\n",
        "\n",
        "        # Sample random batch from memory\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "\n",
        "        # Prepare batch data\n",
        "        states = np.array([experience[0] for experience in minibatch])\n",
        "        actions = np.array([experience[1] for experience in minibatch])\n",
        "        rewards = np.array([experience[2] for experience in minibatch])\n",
        "        next_states = np.array([experience[3] for experience in minibatch])\n",
        "        dones = np.array([experience[4] for experience in minibatch])\n",
        "\n",
        "        # Calculate target Q-values\n",
        "        if self.use_ddqn:\n",
        "            # Double DQN update\n",
        "            q_values_next = self.q_network.predict(next_states, verbose=0)\n",
        "            best_actions = np.argmax(q_values_next, axis=1)\n",
        "            target_q_values = self.target_network.predict(next_states, verbose=0)\n",
        "            targets = rewards + (1 - dones) * self.discount_factor * target_q_values[\n",
        "                np.arange(self.batch_size), best_actions]\n",
        "        else:\n",
        "            # Standard DQN update\n",
        "            target_q_values = self.target_network.predict(next_states, verbose=0)\n",
        "            targets = rewards + (1 - dones) * self.discount_factor * np.amax(\n",
        "                target_q_values, axis=1)\n",
        "\n",
        "        # Update Q-network\n",
        "        q_values = self.q_network.predict(states, verbose=0)\n",
        "        q_values[np.arange(self.batch_size), actions] = targets\n",
        "        self.q_network.fit(states, q_values, verbose=0)\n",
        "\n",
        "        # Decay exploration rate\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        # Update target network periodically\n",
        "        self.train_step_counter += 1\n",
        "        if self.train_step_counter % self.target_update_frequency == 0:\n",
        "            self.update_target_network()"
      ],
      "metadata": {
        "id": "G70Od66ExTYP"
      },
      "id": "G70Od66ExTYP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "20870303",
      "metadata": {
        "id": "20870303"
      },
      "source": [
        "### --- 4. DQNAgent Class --- 1 Mark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7829f4de",
      "metadata": {
        "id": "7829f4de"
      },
      "source": [
        "### --- 5. Main Training Loop --- 1 Mark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_agent(env, agent, num_episodes, max_timesteps_per_episode, render=False):\n",
        "    \"\"\"Train the agent in the environment and track performance metrics\"\"\"\n",
        "    # Initialize tracking metrics\n",
        "    episode_rewards = []\n",
        "    episode_lengths = []\n",
        "    exploration_rates = []\n",
        "    crash_counts = 0\n",
        "    poi_collection_counts = 0\n",
        "\n",
        "    # Store initial weights for comparison (optional)\n",
        "    initial_weights = agent.q_network.get_weights()\n",
        "\n",
        "    # Training loop\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        timestep = 0\n",
        "        episode_crashed = False\n",
        "\n",
        "        for timestep in range(max_timesteps_per_episode):\n",
        "            if render and episode % 100 == 0:  # Render periodically\n",
        "                env.render()\n",
        "\n",
        "            # Select and execute action\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Store experience in replay buffer\n",
        "            agent.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "            # Learn from experiences\n",
        "            agent.learn()\n",
        "\n",
        "            # Update tracking metrics\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            # Check for crash\n",
        "            if reward == BATTERY_CRASH_PENALTY:\n",
        "                episode_crashed = True\n",
        "                crash_counts += 1\n",
        "                break\n",
        "\n",
        "            # Check for POI collection\n",
        "            if reward > TIME_PENALTY + RECHARGE_BONUS:  # POI collected\n",
        "                poi_collection_counts += 1\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Update target network periodically\n",
        "        if episode % agent.target_update_frequency == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "        # Store episode metrics\n",
        "        episode_rewards.append(total_reward)\n",
        "        episode_lengths.append(timestep)\n",
        "        exploration_rates.append(agent.epsilon)\n",
        "\n",
        "        # Print progress\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-10:])\n",
        "            print(f\"Episode {episode+1}/{num_episodes} | \"\n",
        "                  f\"Avg Reward: {avg_reward:.1f} | \"\n",
        "                  f\"Epsilon: {agent.epsilon:.3f} | \"\n",
        "                  f\"POIs Collected: {poi_collection_counts} | \"\n",
        "                  f\"Crashes: {crash_counts}\")\n",
        "\n",
        "    # Training complete\n",
        "    print(\"\\nTraining completed!\")\n",
        "    print(f\"Final Avg Reward (last 100 eps): {np.mean(episode_rewards[-100:]):.1f}\")\n",
        "    print(f\"Total POIs Collected: {poi_collection_counts}\")\n",
        "    print(f\"Total Crashes: {crash_counts}\")\n",
        "\n",
        "    # Return training metrics\n",
        "    return {\n",
        "        'episode_rewards': episode_rewards,\n",
        "        'episode_lengths': episode_lengths,\n",
        "        'exploration_rates': exploration_rates,\n",
        "        'initial_weights': initial_weights,\n",
        "        'final_weights': agent.q_network.get_weights(),\n",
        "        'poi_collections': poi_collection_counts,\n",
        "        'crash_counts': crash_counts\n",
        "    }\n",
        "\n",
        "def plot_trajectory(trajectory, grid_size, charging_stations):\n",
        "    \"\"\"Plot the drone's trajectory\"\"\"\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    x = [step['position'][0] for step in trajectory]\n",
        "    y = [step['position'][1] for step in trajectory]\n",
        "    plt.plot(x, y, marker='o', linestyle='-', label='Drone Trajectory')\n",
        "\n",
        "    # Plot charging stations\n",
        "    for cx, cy in charging_stations:\n",
        "        plt.plot(cx, cy, 's', color='red', markersize=10, label='Charging Station' if (cx, cy) == charging_stations[0] else \"\")\n",
        "\n",
        "    plt.xlim(0, grid_size[0]-1)\n",
        "    plt.ylim(0, grid_size[1]-1)\n",
        "    plt.gca().invert_yaxis() # Invert y-axis to match grid rendering\n",
        "    plt.title('Drone Trajectory during Evaluation')\n",
        "    plt.xlabel('X Position')\n",
        "    plt.ylabel('Y Position')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qlvvrHdqx288"
      },
      "id": "qlvvrHdqx288",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a5d4aff0",
      "metadata": {
        "id": "a5d4aff0"
      },
      "source": [
        "### --- Main Execution Block ---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure TensorFlow to use GPU if available\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if len(physical_devices) > 0:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "    tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
        "    print(\"Using GPU for acceleration\")\n",
        "else:\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "# Initialize environment\n",
        "env = DroneSurveillanceEnv(GRID_SIZE, BATTERY_CAPACITY, INITIAL_BATTERY)\n",
        "\n",
        "# --- Train DQN Agent ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Training DQN Agent\")\n",
        "print(\"=\"*50)\n",
        "dqn_agent = DQNAgent(STATE_SIZE, ACTION_SIZE, LEARNING_RATE, use_ddqn=False)\n",
        "dqn_metrics = train_agent(env, dqn_agent, EPISODES, MAX_TIMESTEPS_PER_EPISODE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lwCf6TpfwIT",
        "outputId": "cb5c62ec-77a4-4ea3-a7c0-37836ccb1ecf"
      },
      "id": "9lwCf6TpfwIT",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CPU\n",
            "\n",
            "==================================================\n",
            "Training DQN Agent\n",
            "==================================================\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 10/200 | Avg Reward: 1.4 | Epsilon: 0.995 | POIs Collected: 4 | Crashes: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Train Double DQN Agent ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Training Double DQN Agent\")\n",
        "print(\"=\"*50)\n",
        "ddqn_agent = DQNAgent(STATE_SIZE, ACTION_SIZE, LEARNING_RATE, use_ddqn=True)\n",
        "ddqn_metrics = train_agent(env, ddqn_agent, EPISODES, MAX_TIMESTEPS_PER_EPISODE)"
      ],
      "metadata": {
        "id": "ntxfZ8jMf2or"
      },
      "id": "ntxfZ8jMf2or",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # # Configure TensorFlow to use GPU if available\n",
        "    # physical_devices = tf.config.list_physical_devices('GPU')\n",
        "    # if len(physical_devices) > 0:\n",
        "    #     tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "    #     tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
        "    #     print(\"Using GPU for acceleration\")\n",
        "    # else:\n",
        "    #     print(\"Using CPU\")\n",
        "\n",
        "    # # Initialize environment\n",
        "    # env = DroneSurveillanceEnv(GRID_SIZE, BATTERY_CAPACITY, INITIAL_BATTERY)\n",
        "\n",
        "    # # --- Train DQN Agent ---\n",
        "    # print(\"\\n\" + \"=\"*50)\n",
        "    # print(\"Training DQN Agent\")\n",
        "    # print(\"=\"*50)\n",
        "    # dqn_agent = DQNAgent(STATE_SIZE, ACTION_SIZE, LEARNING_RATE, use_ddqn=False)\n",
        "    # dqn_metrics = train_agent(env, dqn_agent, EPISODES, MAX_TIMESTEPS_PER_EPISODE)\n",
        "\n",
        "    # # --- Train Double DQN Agent ---\n",
        "    # print(\"\\n\" + \"=\"*50)\n",
        "    # print(\"Training Double DQN Agent\")\n",
        "    # print(\"=\"*50)\n",
        "    # ddqn_agent = DQNAgent(STATE_SIZE, ACTION_SIZE, LEARNING_RATE, use_ddqn=True)\n",
        "    # ddqn_metrics = train_agent(env, ddqn_agent, EPISODES, MAX_TIMESTEPS_PER_EPISODE)\n",
        "\n",
        "    # --- Plotting Results ---\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Plot rewards\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(dqn_metrics['episode_rewards'], label='DQN')\n",
        "    plt.plot(ddqn_metrics['episode_rewards'], label='DDQN')\n",
        "    plt.title('Episode Rewards')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot exploration rates\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(dqn_metrics['exploration_rates'], label='DQN')\n",
        "    plt.plot(ddqn_metrics['exploration_rates'], label='DDQN')\n",
        "    plt.title('Exploration Rate (Epsilon)')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Epsilon')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot moving average rewards\n",
        "    plt.subplot(2, 2, 3)\n",
        "    window_size = 50\n",
        "    plt.plot(np.convolve(dqn_metrics['episode_rewards'], np.ones(window_size)/window_size, mode='valid'),\n",
        "             label='DQN (MA{})'.format(window_size))\n",
        "    plt.plot(np.convolve(ddqn_metrics['episode_rewards'], np.ones(window_size)/window_size, mode='valid'),\n",
        "             label='DDQN (MA{})'.format(window_size))\n",
        "    plt.title('Moving Average Rewards')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Average Reward')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_results.png')\n",
        "    plt.show()\n",
        "\n",
        "    # --- Policy Analysis ---\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Policy Analysis\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Compare final performance\n",
        "    final_dqn_reward = np.mean(dqn_metrics['episode_rewards'][-100:])\n",
        "    final_ddqn_reward = np.mean(ddqn_metrics['episode_rewards'][-100:])\n",
        "    print(f\"\\nFinal Performance (last 100 episodes):\")\n",
        "    print(f\"DQN Average Reward: {final_dqn_reward:.1f}\")\n",
        "    print(f\"DDQN Average Reward: {final_ddqn_reward:.1f}\")\n",
        "    print(f\"Improvement: {((final_ddqn_reward-final_dqn_reward)/final_dqn_reward)*100:.1f}%\")\n",
        "\n",
        "    # Compare crash rates\n",
        "    dqn_crash_rate = dqn_metrics['crash_counts'] / EPISODES * 100\n",
        "    ddqn_crash_rate = ddqn_metrics['crash_counts'] / EPISODES * 100\n",
        "    print(f\"\\nCrash Rates:\")\n",
        "    print(f\"DQN: {dqn_crash_rate:.1f}%\")\n",
        "    print(f\"DDQN: {ddqn_crash_rate:.1f}%\")\n",
        "\n",
        "    # Compare POI collection\n",
        "    print(f\"\\nPOIs Collected:\")\n",
        "    print(f\"DQN: {dqn_metrics['poi_collections']}\")\n",
        "    print(f\"DDQN: {ddqn_metrics['poi_collections']}\")\n",
        "\n",
        "    # --- Evaluation of learned policy ---\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Running Evaluation Episode with DDQN Policy\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Run evaluation with DDQN agent (better performing)\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    trajectory = []\n",
        "\n",
        "    while not done:\n",
        "        # Use greedy policy (epsilon=0)\n",
        "        original_epsilon = ddqn_agent.epsilon\n",
        "        ddqn_agent.epsilon = 0\n",
        "        action = ddqn_agent.choose_action(state)\n",
        "        ddqn_agent.epsilon = original_epsilon\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        trajectory.append({\n",
        "            'position': env.drone_pos.copy(),\n",
        "            'battery': env.battery_level,\n",
        "            'action': action,\n",
        "            'reward': reward,\n",
        "            'disturbance': env.disturbance\n",
        "        })\n",
        "        state = next_state\n",
        "        env.render()\n",
        "        time.sleep(0.1)  # Slow down for visualization\n",
        "\n",
        "    print(f\"\\nEvaluation Episode Results:\")\n",
        "    print(f\"Total Reward: {total_reward}\")\n",
        "    print(f\"Final Battery: {env.battery_level:.1f}%\")\n",
        "    print(f\"POIs Collected: {sum(1 for step in trajectory if step['reward'] > TIME_PENALTY + RECHARGE_BONUS)}\")\n",
        "\n",
        "    # Plot evaluation trajectory\n",
        "    plot_trajectory(trajectory, env.grid_size, env.charging_stations)"
      ],
      "metadata": {
        "id": "6TXQHX8MyiBE"
      },
      "id": "6TXQHX8MyiBE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "001705ed",
      "metadata": {
        "id": "001705ed"
      },
      "source": [
        "### Hyperparameter Tuning & Discussion: (1 Mark)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eeeee5c5",
      "metadata": {
        "id": "eeeee5c5"
      },
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}